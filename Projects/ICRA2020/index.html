<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-9VSBD1WL8J"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-9VSBD1WL8J');
</script>


<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
    body {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight: 300;
        font-size: 18px;
        margin-left: auto;
        margin-right: auto;
        width: 1100px;
    }

    h1 {
        font-weight: 300;
    }

    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
        padding: 20px;
    }

    video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    img.rounded {
        border: 1px solid #eeeeee;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    a:link,
    a:visited {
        color: #1367a7;
        text-decoration: none;
    }

    a:hover {
        color: #208799;
    }

    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
    }

    .layered-paper-big {
        /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
            0px 0px 1px 1px rgba(0, 0, 0, 0.35),
            /* The top layer shadow */
            5px 5px 0 0px #fff,
            /* The second layer */
            5px 5px 1px 1px rgba(0, 0, 0, 0.35),
            /* The second layer shadow */
            10px 10px 0 0px #fff,
            /* The third layer */
            10px 10px 1px 1px rgba(0, 0, 0, 0.35),
            /* The third layer shadow */
            15px 15px 0 0px #fff,
            /* The fourth layer */
            15px 15px 1px 1px rgba(0, 0, 0, 0.35),
            /* The fourth layer shadow */
            20px 20px 0 0px #fff,
            /* The fifth layer */
            20px 20px 1px 1px rgba(0, 0, 0, 0.35),
            /* The fifth layer shadow */
            25px 25px 0 0px #fff,
            /* The fifth layer */
            25px 25px 1px 1px rgba(0, 0, 0, 0.35);
        /* The fifth layer shadow */
        margin-left: 10px;
        margin-right: 45px;
    }


    .layered-paper {
        /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
            0px 0px 1px 1px rgba(0, 0, 0, 0.35),
            /* The top layer shadow */
            5px 5px 0 0px #fff,
            /* The second layer */
            5px 5px 1px 1px rgba(0, 0, 0, 0.35),
            /* The second layer shadow */
            10px 10px 0 0px #fff,
            /* The third layer */
            10px 10px 1px 1px rgba(0, 0, 0, 0.35);
        /* The third layer shadow */
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
    }

    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }

    hr {
        border: 0;
        height: 1.5px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }
</style>

<html>

<head>
    <title>Learning View and Target Invariant Visual Servoing for Navigation</title>
    <meta property="og:title" content="" />
    <meta property="og:image" content="" />
    <link rel="apple-touch-icon" sizes="180x180" href="">
    <link rel="icon" type="image/png" sizes="32x32" href="">
    <link rel="icon" type="image/png" sizes="16x16" href="">
    <link rel="manifest" href="">
    <!-- #TODO -->
    <!-- <meta property="og:url" content="https://www.youtube.com/watch?v=9-Ttb8jsevo" />  -->
</head>

<body>
    <br>
    <center>
        <span style="font-size:42px">Learning View and Target Invariant Visual Servoing<br> for Navigation
        </span>
    </center>

    <br><br>
    <table align=center width=800px>
        <tr>
            <td align=center width=100px>
                <center>
                    <span style="font-size:20px"><a href="https://yimengli46.github.io/">Yimeng Li*</a></span>
                </center>
            </td>

            <td align=center width=100px>
                <center>
                    <span style="font-size:20px"><a href="https://cs.gmu.edu/~kosecka/">Jana Kosecka</a></span>
                </center>
            </td>
        </tr>
    </table>

    <table align=center width=700px>
        <tr>
            <td align=center width=100px>
                <center>
                    <span style="font-size:20px">George Mason University</span>
                </center>
            </td>
        </tr>
    </table>

    <br>
    <table align=center width=700px>
        <tr>
            <td align=center width=100px>
                <center>
                    <span style="font-size:20px">In ICRA 2020</span>
                </center>
            </td>
        </tr>
    </table>

    <br>
    <table align=center width=400px>
        <tr>
            <td align=center width=100px>
                <center>
                    <span style="font-size:20px"><a href="https://arxiv.org/abs/2003.02327">[Paper]</a></span>
                </center>
            </td>

            <!--
            <td align=center width=100px>
                <center>
                    <span style="font-size:20px"><a href="https://arxiv.org/abs/2007.10982">[Arxiv]</a></span>
                </center>
            </td> -->

            <td align=center width=100px>
                <center>
                    <span style="font-size:20px"><a href="https://youtu.be/MdAb3BoB2VA">[Video]</a></span>
                </center>
            </td>

            <!--
            <td align=center width=100px>
                <center>
                    <span style="font-size:20px"><a href="https://github.com/shubham-goel/ds">[Code]</a></span>
                </center>
            </td>
            -->
        </tr>
    </table>

    <br>
    <br>
    <table align=center width=1000px>
        <tr>
            <td width=1000px>
                <center>
                    <video width="900" autoplay muted controls loop>
                        <source src="./resources/images/feature_map.mp4" type="video/mp4">
                    </video>
                </center>
            </td>
        </tr>
    </table>
    <table align=center width=600px>
        <tr>
            <td width=600px>
                <center>
                    <span style="font-size:14px"><i>
                        In this paper, we study the task of training an agent for short-range navigation to the desired location. 
                        The target is either represented as a view image or an image of the target object.
                        In this example, the targeted view is visualized in the first-row first column.
                        </i>
                    </span>
                </center>
            </td>
        </tr>
    </table>

    <br><br>
    The advances in deep reinforcement learning recently revived interest in data-driven learning based approaches to navigation. In this paper we propose to learn viewpoint invariant and target invariant visual servoing for local mobile robot navigation; given an initial view and the goal view or an image of a target, we train deep convolutional network controller for reaching the goal. 
    We present a new architecture for this task which rests on the ability of establishing correspondences between the initial and goal view and novel reward structure motivated by the traditional feedback control error. 
    The advantage of the proposed model is that it does not require calibration and depth information and achieves robust visual servoing in a variety of environments and targets without any parameter fine tuning. We present comprehensive evaluation of the approach and comparison with other deep learning architectures as well as classical visual servoing methods in visually realistic simulation environment. 
    The presented model overcomes the brittleness of classical visual servoing based methods and achieves significantly higher generalization capability compared to the previous learning approaches.
    <br><br>

    <hr>

    <center>
        <h3>Overview</h3>
    </center>
    <table align=center width=900px>
        <tr>
            <td width=600px>
                <center>
                    <div class="video">
                        <iframe width="720" height="405" src="https://youtube.com/embed/MdAb3BoB2VA" frameborder="0"
                            allowfullscreen></iframe>
                    </div>
                </center>
            </td>
        </tr>
    </table>
    <br>

    <center>
        <h3>Approach: train a policy for visual servoing </h3>
    </center>
    <table align=center width=900px>
        <tr>
            <td width=600px>
                <center>
                  <div class="image">
                    <img src='./resources/images/approach.png' width="750" style="border-style: none">
                  </div>
                </center>
            </td>
        </tr>
    </table>
    
    <table align=center width=600px>
        <tr>
            <td width=600px>
                <center>
                    <span style="font-size:14px"><i>
                        We compute Correspondence Map between Current View and Target View and input it into a DQN to predict Q-values for each action.
                        </i>
                    </span>
                </center>
            </td>
        </tr>
    </table>
    <br>

    <center>
        <h3>Visual Servoing Policy Architecture</h3>
    </center>
    <table align=center width=900px>
        <tr>
            <td width=600px>
                <center>
                  <div class="image">
                    <img src='./resources/images/architecture.png' width="750" style="border-style: none">
                  </div>
                </center>
            </td>
        </tr>
    </table>
    <table align=center width=600px>
        <tr>
            <td width=600px>
                <center>
                    <span style="font-size:14px"><i>
                        We model DQN as a deep neural network.
                        The layers can be separated into perception and action in terms of functionality.
                        </i>
                    </span>
                </center>
            </td>
        </tr>
    </table>
    <br>

    <center>
        <h3>ImageGoal Navigation: driving to a target object</h3>
    </center>
    <table align=center width=900px>
        <tr>
            <td width=600px>
                <center>
                    <video width="800" autoplay muted controls loop>
                        <source src="./resources/images/goToObj.mp4" type="video/mp4">
                    </video>
                </center>
            </td>
        </tr>
    </table>
    <table align=center width=600px>
        <tr>
            <td width=600px>
                <center>
                    <span style="font-size:14px"><i>
                        Even though the policy is trained to move to a targeted view, our agent can also drive to a target object without additional training.
Here is one example. We want our agent to go to the couch.
We cut out the couch patch from the initial view and then compute the correspondence between the target object and the current observation.
                        </i>
                    </span>
                </center>
            </td>
        </tr>
    </table>
    <br>

    <hr>
    <!-- <table align=center width=550px> -->
    <table align=center width=800>
        <center>
            <h1>Citation</h1>
        </center>
        <tr>
            
            </td>
            <td>
                <div class="paper" id="assemblies19_bib">
                        <pre xml:space="preserve">
                        @inproceedings{li2020learning,
                          title={Learning view and target invariant visual servoing for navigation},
                          author={Li, Yimeng and Ko{\v{s}}ecka, Jana},
                          booktitle={2020 IEEE International Conference on Robotics and Automation (ICRA)},
                          pages={658--664},
                          year={2020},
                          organization={IEEE}
                        }
                        </pre>
                  </div>
            </td>
        </tr>
    </table>
    <br>

    <hr>

    <table align=center width=1100px>
        <tr>
            <td>
                <left>
                    <center>
                        <h1>Acknowledgements</h1>
                        We thank members of the <a href="https://github.com/GMU-vision-robotics">GMU Vision and Robotics Lab</a>. 
                        <br>This webpage template was borrowed from some <a
                            href="https://richzhang.github.io/colorization/">colorful folks</a>.
                    </center>
                </left>
            </td>
        </tr>
    </table>

    <br><br>
</body>

</html>