<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-9VSBD1WL8J"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-9VSBD1WL8J');
</script>


<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
    body {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight: 300;
        font-size: 18px;
        margin-left: auto;
        margin-right: auto;
        width: 1100px;
    }

    h1 {
        font-weight: 300;
    }

    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
        padding: 20px;
    }

    video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    img.rounded {
        border: 1px solid #eeeeee;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    a:link,
    a:visited {
        color: #1367a7;
        text-decoration: none;
    }

    a:hover {
        color: #208799;
    }

    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
    }

    .layered-paper-big {
        /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
            0px 0px 1px 1px rgba(0, 0, 0, 0.35),
            /* The top layer shadow */
            5px 5px 0 0px #fff,
            /* The second layer */
            5px 5px 1px 1px rgba(0, 0, 0, 0.35),
            /* The second layer shadow */
            10px 10px 0 0px #fff,
            /* The third layer */
            10px 10px 1px 1px rgba(0, 0, 0, 0.35),
            /* The third layer shadow */
            15px 15px 0 0px #fff,
            /* The fourth layer */
            15px 15px 1px 1px rgba(0, 0, 0, 0.35),
            /* The fourth layer shadow */
            20px 20px 0 0px #fff,
            /* The fifth layer */
            20px 20px 1px 1px rgba(0, 0, 0, 0.35),
            /* The fifth layer shadow */
            25px 25px 0 0px #fff,
            /* The fifth layer */
            25px 25px 1px 1px rgba(0, 0, 0, 0.35);
        /* The fifth layer shadow */
        margin-left: 10px;
        margin-right: 45px;
    }


    .layered-paper {
        /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
            0px 0px 1px 1px rgba(0, 0, 0, 0.35),
            /* The top layer shadow */
            5px 5px 0 0px #fff,
            /* The second layer */
            5px 5px 1px 1px rgba(0, 0, 0, 0.35),
            /* The second layer shadow */
            10px 10px 0 0px #fff,
            /* The third layer */
            10px 10px 1px 1px rgba(0, 0, 0, 0.35);
        /* The third layer shadow */
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
    }

    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }

    hr {
        border: 0;
        height: 1.5px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }
</style>

<html>

<head>
    <title>Learning-Augmented Model-Based Planning for Visual Exploration</title>
    <meta property="og:title" content="" />
    <meta property="og:image" content="" />
    <link rel="apple-touch-icon" sizes="180x180" href="">
    <link rel="icon" type="image/png" sizes="32x32" href="">
    <link rel="icon" type="image/png" sizes="16x16" href="">
    <link rel="manifest" href="">
    <!-- #TODO -->
    <!-- <meta property="og:url" content="https://www.youtube.com/watch?v=9-Ttb8jsevo" />  -->
</head>

<body>
    <br>
    <center>
        <span style="font-size:42px">Learning-Augmented Model-Based Planning<br> for Visual Exploration
        </span>

    </center>

    <br><br>
    <table align=center width=800px>
        <tr>
            <td align=center width=100px>
                <center>
                    <span style="font-size:20px"><a href="https://yimengli46.github.io/">Yimeng Li*</a></span>
                </center>
            </td>

            <td align=center width=100px>
                <center>
                    <span style="font-size:20px"><a href="">Arnab Debnath*</a></span>
                </center>
            </td>

            <td align=center width=100px>
                <center>
                    <span style="font-size:20px"><a href="https://cs.gmu.edu/~gjstein/">Gregory Stein</a></span>
                </center>
            </td>

            <td align=center width=100px>
                <center>
                    <span style="font-size:20px"><a href="https://cs.gmu.edu/~kosecka/">Jana Kosecka</a></span>
                </center>
            </td>
        </tr>
    </table>

    <table align=center width=700px>
        <tr>
            <td align=center width=100px>
                <center>
                    <span style="font-size:20px">George Mason University</span>
                </center>
            </td>
        </tr>
    </table>

    <br>
    <table align=center width=700px>
        <tr>
            <td align=center width=100px>
                <center>
                    <span style="font-size:20px">IROS 2023</span>
                </center>
            </td>
        </tr>
    </table>

    <br>
    <table align=center width=400px>
        <tr>
            <td align=center width=100px>
                <center>
                    <span style="font-size:20px"><a href="https://arxiv.org/abs/2211.07898">[Paper]</a></span>
                </center>
            </td>

            <td align=center width=100px>
                <center>
                    <span style="font-size:20px"><a href="https://youtu.be/wOwRMQJZM68">[Video]</a></span>
                </center>
            </td>

            <td align=center width=100px>
                <center>
                    <span style="font-size:20px"><a href="https://github.com/yimengli46/bellman-exploration">[Code]</a></span>
                </center>
            </td>

        </tr>
    </table>

    <br>
    <br>
    <table align=center width=1000px>
        <tr>
            <td width=1000px>
                <center>
                    <!-- img src="./resources/images/approach.png" width="1000px"></img>-->
                    <video width="469" autoplay muted controls loop>
                        <source src="./resources/images/DP_large_merged.mp4" type="video/mp4">
                    </video>
                </center>
            </td>
        </tr>
    </table>

    <table align=center width=600px>
        <tr>
            <td width=600px text-align: left>
                
                    <span style="font-size:18px; display: block; margin-top: 10px;" >
At each step, the agent will receive a 360 panorama or 90 degree egocentric observation.
Here in the video, we show a running example of our model exploring a novel scene using a frontier-based approach.
A frontier is a boundary between free and unknown space, as denoted by the green pixels in the video. 
Yellow pixels are the selected frontier.
                    </span>
                
            </td>
        </tr>
    </table>

    <br><br>
    We consider the problem of time-limited robotic exploration in previously unseen environments where explo- ration is limited by a predefined amount of time. We propose a novel exploration approach using learning-augmented model- based planning. We generate a set of subgoals associated with frontiers on the current map and derive a Bellman Equation for exploration with these subgoals. Visual sensing and advances in semantic mapping of indoor scenes are exploited for training a deep convolutional neural network to estimate properties associated with each frontier: the expected unobserved area beyond the frontier and the expected timesteps (discretized actions) required to explore it. The proposed model-based planner is guaranteed to explore the whole scene if time permits. We thoroughly evaluate our approach on a large-scale pseudo-realistic indoor dataset (Matterport3D) with the Habitat simulator. We compare our approach with classical and more recent RL-based exploration methods, demonstrating its clear advantages in several settings.
    <br><br>

    <hr>

    <center>
        <h3>Overview</h3>
    </center>
    <table align=center width=900px>
        <tr>
            <td width=600px>
                <center>
                    <div class="video">
                        <iframe width="720" height="405" src="https://www.youtube.com/embed/vTr01A6Jna4" frameborder="0"
                            allowfullscreen></iframe>
                    </div>
                </center>
            </td>
        </tr>
    </table>
    <br>

    <center>
        <h3>Learning-Augmented Model-Based Frontier Exploration (LFE)</h3>
    </center>

    <table align=center width=900px>
        <tr>
            <td width=600px>
                <center>
                  <div class="image">
                    <img src='./resources/images/approach.png' width="785" style="border-style: none">
                  </div>
                </center>

                <span style="font-size:18px;display: block; margin-top: 10px;" >
                    This diagram gives an overview of our learning-augmented model-based exploration algorithm, which allows us to compute the expected value of each action in a computationally feasible way for planning through an unknown environment. We use learning modules to estimate the terms R_A, D_in and D_out, thereby introducing prior knowledge about environment regularities into the decision-making procedure.
                 </span>
            </td>
        </tr>
    </table>
    <br>

    <center>
        <h3>Architecture of the entire exploration system</h3>
    </center>
    <table align=center width=900px>
        <tr>
            <td width=600px>
                <center>
                  <div class="image">
                    <img src='./resources/images/modules.png' width="1200" style="border-style: none">
                  </div>
                </center>
                <span style="font-size:18px; display: block; margin-top: 10px;" >
The detected and the selected frontiers are drawn in green and yellow respectively. Our proposed
Learning-augmented model-based frontier-based exploration is used for the frontier selection module. We design two learning modules to estimate the
frontier properties. The UNet model takes in the local occupancy map and local semantic map and estimates values for all the visible frontiers on the map.
The ResNet-18 model takes in the egocentric depth and semantic observation corresponding to a frontier and estimates the values only for this frontier.
                </span>
                
            </td>
        </tr>
    </table>
    <br>

    <center>
        <h3>Compare different planning strategies</h3>
    </center>
    <table align=center width=900px>
        <tr>
            <td width=600px>
                <center>
                  <div class="image">
                    <img src='./resources/images/planning_strategies.png' width="700" style="border-style: none">
                  </div>
                </center>

                <span style="font-size:18px;display: block; margin-top: 10px;" >
                    In frontier-based methods, the robotâ€™s actions correspond to navigation to frontiers.
These actions rely on the partial map to plan and reliably navigate through space the robot has already seen. 
Existing Approaches greedily select the next frontier-action.

Classical non-learning greedy exploration from Brian greedily select the next frontier-action via simple heuristics by navigating to the nearest frontier.
Recent work called PONI uses visual sensing and replaces simple heuristics with a learning-based module.
                </span>
            </td>
        </tr>
    </table>
    <br>

    <center>
        <h3>Learning Module</h3>
    </center>
    <table align=center width=900px>
        <tr>
            <td width=600px>
                <center>
                  <div class="image">
                    <img src='./resources/images/unet_resnet.png' width="500" style="border-style: none">
                  </div>
                </center>

                <span style="font-size:18px; display: block; margin-top: 10px;" >
                    We develop a learning module for estimating frontier properties from a partial map or egocentric views.<br>
For the map based model, We use a U-net architecture where the input to the U-Net model is the currently observed occupancy and semantic map.
For the view based model, the input to the ResNet-18 model is the egocentric depth and semantic observation.
                </span>
            </td>
        </tr>
    </table>
    <br>

    <center>
        <h3>Perception Module</h3>
    </center>
    <table align=center width=900px>
        <tr>
            <td width=600px>
                <center>
                  <div class="image">
                    <img src='./resources/images/perception_module.png' width="600" style="border-style: none">
                  </div>
                </center>
            </td>
        </tr>
    </table>
    <br>

    <center>
        <h3>Results: compare LFE with the baseline Greedy approach</h3>
    </center>
    <table align=center width=900px>
        <tr>
            <td width=600px>
                <center>
                  <div class="image">
                    <img src='./resources/images/example_traj.png' width="759" style="border-style: none">
                  </div>
                </center>
            </td>
        </tr>
    </table>
    <br>

    <hr>
    <!-- <table align=center width=550px> -->
    <table align=center width=800>
        <center>
            <h1>Citation</h1>
        </center>
        <tr>
            
            </td>
            <td>
                <div class="paper" id="assemblies19_bib">
                        <pre xml:space="preserve">
                        @inproceedings{li2023learning,
                          title={Learning-augmented model-based planning for visual exploration},
                          author={Li, Yimeng and Debnath, Arnab and Stein, Gregory J and Ko{\v{s}}eck{\'a}, Jana},
                          booktitle={2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
                          pages={5165--5171},
                          year={2023},
                          organization={IEEE}
                        }
                        </pre>
                  </div>
            </td>
        </tr>
    </table>
    <br>

    <hr>

    <table align=center width=1100px>
        <tr>
            <td>
                <left>
                    <center>
                        <h1>Acknowledgements</h1>
                        We thank members of the <a href="https://github.com/GMU-vision-robotics">GMU Vision and Robotics Lab</a> and <a href="https://cs.gmu.edu/~gjstein/">RAIL</a>. 
                        <br>This webpage template was borrowed from some <a
                            href="https://richzhang.github.io/colorization/">colorful folks</a>.
                    </center>
                </left>
            </td>
        </tr>
    </table>

    <br><br>
</body>

</html>