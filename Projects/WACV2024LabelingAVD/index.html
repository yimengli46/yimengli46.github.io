<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-9VSBD1WL8J"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-9VSBD1WL8J');
</script>


<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
    body {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight: 300;
        font-size: 18px;
        margin-left: auto;
        margin-right: auto;
        width: 1100px;
    }

    h1 {
        font-weight: 300;
    }

    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
        padding: 20px;
    }

    video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    img.rounded {
        border: 1px solid #eeeeee;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    a:link,
    a:visited {
        color: #1367a7;
        text-decoration: none;
    }

    a:hover {
        color: #208799;
    }

    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
    }

    .layered-paper-big {
        /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
            0px 0px 1px 1px rgba(0, 0, 0, 0.35),
            /* The top layer shadow */
            5px 5px 0 0px #fff,
            /* The second layer */
            5px 5px 1px 1px rgba(0, 0, 0, 0.35),
            /* The second layer shadow */
            10px 10px 0 0px #fff,
            /* The third layer */
            10px 10px 1px 1px rgba(0, 0, 0, 0.35),
            /* The third layer shadow */
            15px 15px 0 0px #fff,
            /* The fourth layer */
            15px 15px 1px 1px rgba(0, 0, 0, 0.35),
            /* The fourth layer shadow */
            20px 20px 0 0px #fff,
            /* The fifth layer */
            20px 20px 1px 1px rgba(0, 0, 0, 0.35),
            /* The fifth layer shadow */
            25px 25px 0 0px #fff,
            /* The fifth layer */
            25px 25px 1px 1px rgba(0, 0, 0, 0.35);
        /* The fifth layer shadow */
        margin-left: 10px;
        margin-right: 45px;
    }


    .layered-paper {
        /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
            0px 0px 1px 1px rgba(0, 0, 0, 0.35),
            /* The top layer shadow */
            5px 5px 0 0px #fff,
            /* The second layer */
            5px 5px 1px 1px rgba(0, 0, 0, 0.35),
            /* The second layer shadow */
            10px 10px 0 0px #fff,
            /* The third layer */
            10px 10px 1px 1px rgba(0, 0, 0, 0.35);
        /* The third layer shadow */
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
    }

    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }

    hr {
        border: 0;
        height: 1.5px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }
</style>

<html>

<head>
    <title>Labeling Indoor Scenes with Fusion of Out-of-the-Box Perception Models</title>
    <meta property="og:title" content="" />
    <meta property="og:image" content="" />
    <link rel="apple-touch-icon" sizes="180x180" href="">
    <link rel="icon" type="image/png" sizes="32x32" href="">
    <link rel="icon" type="image/png" sizes="16x16" href="">
    <link rel="manifest" href="">
    <!-- #TODO -->
    <!-- <meta property="og:url" content="https://www.youtube.com/watch?v=9-Ttb8jsevo" />  -->
</head>

<body>
    <br>
    <center>
        <span style="font-size:42px">Labeling Indoor Scenes with Fusion of<br> Out-of-the-Box Perception Models
        </span>

    </center>

    <br><br>
    <table align=center width=800px>
        <tr>
            <td align=center width=100px>
                <center>
                    <span style="font-size:20px"><a href="https://yimengli46.github.io/">Yimeng Li*</a></span>
                </center>
            </td>

            <td align=center width=100px>
                <center>
                    <span style="font-size:20px"><a href="https://navidnr.com/">Navid Rajabi*</a></span>
                </center>
            </td>

            <td align=center width=100px>
                <center>
                    <span style="font-size:20px"><a href="https://scholar.google.com/citations?user=pZkPBhkAAAAJ&hl=en">Sulabh Shrestha</a></span>
                </center>
            </td>

            <td align=center width=100px>
                <center>
                    <span style="font-size:20px"><a href="https://analytics.drake.edu/~reza/">Md Alimoor Reza</a></span>
                </center>
            </td>

            <td align=center width=100px>
                <center>
                    <span style="font-size:20px"><a href="https://cs.gmu.edu/~kosecka/">Jana Kosecka</a></span>
                </center>
            </td>
        </tr>
    </table>

    <table align=center width=700px>
        <tr>
            <td align=center width=100px>
                <center>
                    <span style="font-size:20px">George Mason University, Drake University</span>
                </center>
            </td>
        </tr>
    </table>

    <br>
    <table align=center width=700px>
        <tr>
            <td align=center width=100px>
                <center>
                    <span style="font-size:20px">WACV 2024 2nd Workshop on Pretraining</span>
                </center>
            </td>
        </tr>
    </table>

    <br>
    <table align=center width=400px>
        <tr>
            <td align=center width=100px>
                <center>
                    <span style="font-size:20px"><a href="https://arxiv.org/abs/2311.10883">[Paper]</a></span>
                </center>
            </td>

            <!--
            <td align=center width=100px>
                <center>
                    <span style="font-size:20px"><a href="">[Video]</a></span>
                </center>
            </td>
        -->

            <td align=center width=100px>
                <center>
                    <span style="font-size:20px"><a href="https://github.com/yimengli46/auto_sseg_avd_main">[Code]</a></span>
                </center>
            </td>

        </tr>
    </table>

    <br>
    <br>
    <table align=center width=1000px>
        <tr>
            <td width=1000px>
                <center>
                    <img src="./resources/images/AutoLabeling_Tasks.png" width="1200px"></img>
                </center>
            </td>
        </tr>
    </table>

    <table align=center width=1000px>
        <tr>
            <td width=600px text-align: left>
                
                    <span style="font-size:18px; display: block; margin-top: 10px;" >
Starting from an RGB-D dataset, we propose a labeling approach for semantic segmentation annotations. 
On top of the semantic segmentation results, we additionally proposed two downstream tasks for robot navigation. 
We build top-down-view semantic maps and use them for zero-shot semantic-goal navigation. 
We proposed an object part segmentation task for the ’cabinet handle’ related to the robot mobile manipulation task.
                    </span>
                
            </td>
        </tr>
    </table>

    <br><br>
    The image annotation stage is a critical and often the most time-consuming part required for training and evaluating object detection and semantic segmentation models. Deployment of the existing models in novel environments often requires detecting novel semantic classes not present in the training data. Furthermore, indoor scenes contain significant viewpoint variations, which need to be handled properly by trained perception models. We propose to leverage the recent advancements in state-of-the-art models for bottom-up segmentation (SAM), object detection (Detic), and semantic segmentation (MaskFormer), all trained on large-scale datasets. We aim to develop a cost-effective labeling approach to obtain pseudo-labels for semantic segmentation and object instance detection in indoor environments, with the ultimate goal of facilitating the training of lightweight models for various downstream tasks. We also propose a multi-view labeling fusion stage, which considers the setting where multiple views of the scenes are available and can be used to identify and rectify single-view inconsistencies. We demonstrate the effectiveness of the proposed approach on the Active Vision dataset and the ADE20K dataset. We evaluate the quality of our labeling process by comparing it with human annotations. Also, we demonstrate the effectiveness of the obtained labels in downstream tasks such as object goal navigation and part discovery. In the context of object goal navigation, we depict enhanced performance using this fusion approach compared to a zero-shot baseline that utilizes large monolithic vision-language pre-trained models.
    <br><br>

    <hr>

    <!--
    <center>
        <h3>Overview</h3>
    </center>
    <table align=center width=900px>
        <tr>
            <td width=600px>
                <center>
                    <div class="video">
                        <iframe width="720" height="405" src="https://www.youtube.com/embed/v" frameborder="0"
                            allowfullscreen></iframe>
                    </div>
                </center>
            </td>
        </tr>
    </table>
    <br>
-->

    <center>
        <h3>Single-View Labeling</h3>
    </center>

    <table align=center width=900px>
        <tr>
            <td width=600px>
                <center>
                  <div class="image">
                    <img src='./resources/images/approach.jpg' width="1200" style="border-style: none">
                  </div>
                </center>

                <span style="font-size:18px;display: block; margin-top: 10px;" >
                    This diagram gives an overview of our labeling approach at the single-view labeling stage. Given an input image, we generate masks for foreground classes utilizing Detic and SAM at steps (A) and (B). If any classes with manual bounding boxes are available, we generate masks utilizing SAM at steps (C) and (D). We generate masks for the entire image utilizing MaskFormer and SAM at steps (E) and (F). We overlay the results of foreground class masks on top of the entire image’s mask and achieve the semantic segmentation and instance segmentation annotations.
                 </span>
            </td>
        </tr>
    </table>
    <br>

    <center>
        <h3>MaskFormer and SAM</h3>
    </center>
    <table align=center width=900px>
        <tr>
            <td width=600px>
                <center>
                  <div class="image">
                    <img src='./resources/images/maskformer_vis.png' width="800" style="border-style: none">
                  </div>
                </center>
                <span style="font-size:18px; display: block; margin-top: 10px;" >
                An example of labeling using Semantic Segmentation (MaskFormer) and SAM. MaskFormer produces good predictions for background classes but not so well for foreground classes: the coffee machine is misclassified as a ‘stove’ (black bounding box), and the cooking pot (cyan bounding box) is missed entirely.
                </span>
                
            </td>
        </tr>
    </table>
    <br>

    <center>
        <h3>Object Detector (Detic) and SAM</h3>
    </center>
    <table align=center width=900px>
        <tr>
            <td width=600px>
                <center>
                  <div class="image">
                    <img src='./resources/images/detic_vis.png' width="700" style="border-style: none">
                  </div>
                </center>

                <span style="font-size:18px;display: block; margin-top: 10px;" >
                We employ Detic with LVIS vocabulary, which includes 1203 classes and covers many known indoor object classes. We utilize SAM to generate high-quality masks
by using the bounding boxes as input prompts. In practice,
we prompt SAM with bounding boxes and a point corresponding to the centroid of the masks from Detic, leading
to high-quality object instance segmentation shown in the figure.
                </span>
            </td>
        </tr>
    </table>
    <br>

    <center>
        <h3>Multiview Verification</h3>
    </center>
    <table align=center width=900px>
        <tr>
            <td width=600px>
                <center>
                  <div class="image">
                    <img src='./resources/images/mv.png' width="800" style="border-style: none">
                  </div>
                </center>

                <span style="font-size:18px; display: block; margin-top: 10px;" >
                     An example of Multiview Verification. The refrigerator
(cyan bounding box) in view Ak is originally labeled as wall and
missing mask for some of its parts. The error is resolved by fusing
labels from views Am and An with the correct annotation for the
class.
                </span>
            </td>
        </tr>
    </table>
    <br>

    <center>
        <h3>Object Part Discovery</h3>
    </center>
    <table align=center width=900px>
        <tr>
            <td width=600px>
                <center>
                  <div class="image">
                    <img src='./resources/images/exp_2_approach.jpg' width="1200" style="border-style: none">
                  </div>
                </center>

                <span style="font-size:18px; display: block; margin-top: 10px;">
                    This diagram gives an overview of the labeling cabinet handle. We choose SAM segments within the detected ’cabinet handle’
bounding box. Then, we extract ResNet50 features for these segments. We cluster these feature points through KMeans and manually label
the cluster containing ’cabinet handle’ points. We backproject the label to the original image to get the ’cabinet handle’ annotations
                </span>
            </td>
        </tr>
    </table>
    <br>

    <center>
        <h3>Results: Object Goal Navigation</h3>
    </center>
    <table align=center width=800px>
        <tr>
            <td width=800px>
                <center>
                  <div class="image">
                    <img src='./resources/images/vis_005.png' width="800" style="border-style: none">
                  </div>
                </center>

                 <span style="font-size:18px; display: block; margin-top: 10px;">
                    A qualitative result of an episode for navigating to a kitchen sink in Home-005. Our approach successfully drives the agent to the sink, while the VL-Map approach fails to approach the target object.
                </span>
            </td>
        </tr>
    </table>

    <table align=center width=800px>
        <tr>
            <td width=800px>
                <center>
                  <div class="image">
                    <img src='./resources/images/vis_nav_006.png' width="800" style="border-style: none">
                  </div>
                </center>

                 <span style="font-size:18px; display: block; margin-top: 10px;">
                    A qualitative result of an episode for navigating to a flowerpot in Home-006. Both approaches successfully reach the target object.
                </span>
            </td>
        </tr>
    </table>
    <br>

    <hr>
    <!-- <table align=center width=550px> -->
    <table align=center width=800>
        <center>
            <h1>Citation</h1>
        </center>
        <tr>
            
            </td>
            <td>
                <div class="paper" id="assemblies19_bib">
                        <pre xml:space="preserve">
                        @inproceedings{li2024labeling,
                        title={Labeling Indoor Scenes with Fusion of Out-of-the-Box Perception Models},
                        author={Li, Yimeng and Rajabi, Navid and Shrestha, Sulabh and Alimoor, Reza and Ko{\v{s}}eck{\'a}, Jana},
                        booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
                        pages={578--587},
                        year={2024}
                        }
                        </pre>
                  </div>
            </td>
        </tr>
    </table>
    <br>

    <hr>

    <table align=center width=1100px>
        <tr>
            <td>
                <left>
                    <center>
                        <h1>Acknowledgements</h1>
                        We thank members of the <a href="https://github.com/GMU-vision-robotics">GMU Vision and Robotics Lab</a>. 
                        <br>This webpage template was borrowed from some <a
                            href="https://richzhang.github.io/colorization/">colorful folks</a>.
                    </center>
                </left>
            </td>
        </tr>
    </table>

    <br><br>
</body>

</html>